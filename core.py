import json
import time
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from send import NotificationSender
import os
from pymongo import MongoClient
import cfscrape
import shutil
from dotenv import load_dotenv
from urllib.parse import urlparse
from msgparse import thread_message, comment_message
import curl_cffi
from filter import Filter

# Load variables from data/.env
load_dotenv('data/.env')


scraper = cfscrape.create_scraper()


class ForumMonitor:
    def __init__(self, config_path='data/config.json'):
        self.config_path = config_path
        self.mongo_host = os.getenv("MONGO_HOST", 'mongodb://localhost:27017/')
        self.load_config()
        
        self.mongo_client = MongoClient(self.mongo_host)
        self.db = self.mongo_client['forum_monitor']
        self.threads = self.db['threads']
        self.comments = self.db['comments']

        self.threads.create_index('link', unique=True)
        self.comments.create_index('comment_id', unique=True)

    # 简化版当前时间调用函数
    def current_time(self):
        return datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")

    # 简化配置加载
    def load_config(self):
        # 如果配置文件不存在，复制示例文件
        if not os.path.exists(self.config_path):
            shutil.copy('example.json', self.config_path)
        with open(self.config_path, 'r') as f:
            self.config = json.load(f)['config']
        self.notifier = NotificationSender(self.config)
        self.filter = Filter(self.config)
        print("配置文件加载成功")


    # -------- RSS LET/LES -----------
    def check_lets(self, urls):
        for url in urls:
            domain = url.split("//")[1].split(".")[0]
            category = url.split("/")[4]
            # 当前时间
            print(f"[{self.current_time()}] 检查 {domain} {category} RSS...")
            res = scraper.get(url)
            if res.status_code != 200:
                print(f"获取 {domain} 失败")
                return
            soup = BeautifulSoup(res.text, 'xml')
            for item in soup.find_all('item')[:6]:
                self.convert_rss(item, domain, category)

    # -------- EXTRA URLS -----------
    def check_extra_urls(self, urls):
        for url in urls:
            print(f"[{self.current_time()}] 检查 extra URL: {url}")
            # 检查数据库是否已存在
            thread = self.threads.find_one({'link': url})
            if thread:
                self.fetch_comments(thread)
            # 不存在则抓取并插入
            self.fetch_thread_page(url)

    # 将 RSS item 转成 thread_data
    def convert_rss(self, item, domain, category):
        title = item.find('title').text
        link = item.find('link').text
        desc = BeautifulSoup(item.find('description').text, 'lxml').text
        creator = item.find('dc:creator').text
        pub_date = datetime.strptime(item.find('pubDate').text, "%a, %d %b %Y %H:%M:%S +0000")

        thread_data = {
            'domain': domain,
            'category': category,
            'title': title,
            'link': link,
            'description': desc,
            'creator': creator,
            'pub_date': pub_date,
            'created_at': datetime.now(timezone.utc),
            'last_page': 1
        }

        self.handle_thread(thread_data)
        self.fetch_comments(thread_data)

    # -------- 线程存储 + 通知 --------
    def handle_thread(self, thread):
        exists = self.threads.find_one({'link': thread['link']})
        if exists:
            return

        self.threads.insert_one(thread)
        # 发布时间 24h 内才推送
        if (datetime.now(timezone.utc) - thread['pub_date'].replace(tzinfo=timezone.utc)).total_seconds() <= 86400:
            if self.config.get('use_ai_filter', False):
                ai_description = self.filter.ai_filter(thread['description'],self.config['thread_prompt'])
                if 'false' in ai_description.lower():
                    return
            else:
                ai_description = ""
            msg = thread_message(thread, ai_description)
            self.notifier.send_message(msg)

    # 新增：直接抓取单个线程页面并解析成 thread_data 格式
    def fetch_thread_page(self, url):
        # res = scraper.get(url)
        res = curl_cffi.get(url,impersonate="chrome124")
        if res.status_code != 200:
            print(f"获取页面失败 {url} 状态码 {res.status_code}")
            return None

        soup = BeautifulSoup(res.text, "html.parser")

        item_header = soup.select_one("div.Item-Header.DiscussionHeader")
        page_title = soup.select_one("#Item_0.PageTitle")

        if not item_header or not page_title:
            print("结构不匹配")
            return None

        title = page_title.select_one("h1")
        title = title.text.strip() if title else ""

        creator = item_header.select_one(".Author .Username")
        creator = creator.text.strip() if creator else ""

        time_el = item_header.select_one("time")
        if time_el and time_el.has_attr("datetime"):
            pub_date_str = time_el["datetime"]
            try:
                pub_date = datetime.strptime(pub_date_str, "%Y-%m-%dT%H:%M:%S+00:00")
            except ValueError:
                pub_date = datetime.now(timezone.utc)  # 如果解析失败，使用当前时间
        else:
            pub_date = datetime.now(timezone.utc)

        category = item_header.select_one(".Category a")
        category = category.text.strip() if category else ""

        desc_el = soup.select_one(".Message.userContent")
        description = desc_el.get_text("\n", strip=True) if desc_el else ""

        parsed = urlparse(url)
        domain = url.split("//")[1].split(".")[0]

        thread_data = {
            "domain": domain,
            "category": category,
            "title": title,
            "link": url,
            "description": description,
            "creator": creator,
            "pub_date": pub_date,
            "created_at": datetime.now(timezone.utc),
            "last_page": 1
        }

        self.handle_thread(thread_data)
        self.fetch_comments(thread_data)


    # -------- 评论抓取统一逻辑（LET / LES 一样） --------
    def fetch_comments(self, thread):
        last_page = self.threads.find_one({'link': thread['link']}).get('last_page', 1)
        if last_page < 1:
            last_page = 1
        while True:
            page_url = f"{thread['link']}/p{last_page}"
            print(f"获取评论页面 {page_url} ...")
            # res = scraper.get(page_url)
            res = curl_cffi.get(page_url,impersonate="chrome124")
            if res.status_code != 200:
                if res.status_code == 404:
                    # 更新 last_page
                    self.threads.update_one(
                        {'link': thread['link']},
                        {'$set': {'last_page': last_page-1}}
                    )
                    break
                else:
                    print(f"获取评论失败 {page_url} 状态码 {res.status_code}")
                    break
            self.parse_comments(res.text, thread)
            last_page += 1
            time.sleep(1)

    # -------- 通用评论解析 --------
    def parse_comments(self, html, thread):
        soup = BeautifulSoup(html, 'html.parser')
        items = soup.find_all('li', class_='ItemComment')

        for it in items:
            cid = it.get('id')
            if not cid:
                continue
            cid = cid.split('_')[1]

            author = it.find('a', class_='Username').text
            role = it.find('span', class_='RoleTitle').text if it.find('span', class_='RoleTitle') else None
            # msg = it.find('div', class_='Message').text.strip()
            message_div = it.find('div', class_='Message')
            if message_div:
                msg_parts = []
                for element in message_div.children:
                    if element.name == 'blockquote' and 'UserQuote' in element.get('class', []):
                        # 这是引用内容
                        quote_text = element.get_text(strip=True)
                        msg_parts.append(f"[Quote]{quote_text}[/Quote]")
                    elif element.name and element.name in ['p', 'div']:
                        # 这是新内容
                        text = element.get_text(strip=True)
                        if text:
                            msg_parts.append(text)
                    elif isinstance(element, str):
                        # 文本节点
                        text = element.strip()
                        if text:
                            msg_parts.append(text)
                msg = '\n'.join(msg_parts)
            else:
                msg = ""
            created = it.find('time')['datetime']
            if self.config.get('comment_filter') == 'by_role':
                # by_role 过滤器，为 None '' 或者只有 member 则跳过
                if not role or role.strip().lower() == 'member':
                    continue
            if self.config.get('comment_filter') == 'by_author':
                # 只监控作者自己的后续更新
                if author != thread['creator']:
                    continue

            comment = {
                'comment_id': f"{thread['domain']}_{cid}",
                'thread_url': thread['link'],
                'author': author,
                'message': msg.strip(),  # 修改处：去掉了 [:200]
                'created_at': datetime.strptime(created, "%Y-%m-%dT%H:%M:%S+00:00"),
                'created_at_recorded': datetime.now(timezone.utc),
                'url': f"https://{thread['domain']}.com/discussion/comment/{cid}/#Comment_{cid}"
            }

            self.handle_comment(comment, thread)

    # -------- 存储评论 + 通知 --------
    def handle_comment(self, comment, thread):
        if self.comments.find_one({'comment_id': comment['comment_id']}):
            return

        self.comments.update_one({'comment_id': comment['comment_id']},
                                 {'$set': comment}, upsert=True)
        # 只推送 24 小时内的
        if (datetime.now(timezone.utc) - comment['created_at'].replace(tzinfo=timezone.utc)).total_seconds() <= 86400:
            if self.config.get('use_keywords_filter', False) and (not self.filter.keywords_filter(comment['message'], self.config.get('keywords_rule', ''))):
                    return
            if self.config.get('use_ai_filter', False):
                ai_description = self.filter.ai_filter(comment['message'],self.config['comment_prompt'])
                if 'false' in ai_description.lower():
                    return
            else:
                ai_description = ""
            msg = comment_message(thread, comment, ai_description)
            self.notifier.send_message(msg)

    # -------- 主循环 --------
    def start_monitoring(self):
        print("开始监控...")
        freq = self.config.get('frequency', 600)

        while True:
            self.check_extra_urls(urls=self.config.get('extra_urls', []))
            if not self.config.get('only_extra', False):
                # 处理 RSS 和 extra URLs
                self.check_lets(urls=self.config.get('urls', [
                    "https://lowendspirit.com/categories/offers/feed.rss",
                    "https://lowendtalk.com/categories/offers/feed.rss"
                ]))
            print(f"[{self.current_time()}] 遍历结束，休眠 {freq} 秒...")
            time.sleep(freq)

    # 外部重载配置方法
    def reload(self):
        print("重新加载配置...")
        self.load_config()
        
if __name__ == "__main__":
    monitor = ForumMonitor()
    monitor.start_monitoring()
